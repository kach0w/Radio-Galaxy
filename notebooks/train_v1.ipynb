{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (2.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kar\\anaconda3\\envs\\asdfs\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchsummary\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import os\n",
    "\n",
    "from MiraBest_Goof import MiraBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = MiraBest(root='../batches', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = MiraBest(root='../batches', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "test_len = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    x, y = data\n",
    "    test_len += x.shape[0]\n",
    "train_len = 0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    x, y = data\n",
    "    train_len += x.shape[0]\n",
    "print(test_len)\n",
    "print(train_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\"train\": train_len, \"test\": test_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3)\n",
    "        self.conv3 = nn.Conv2d(3, 6, 3)\n",
    "        # self.fc1 = nn.Linear(16 * 73 * 73, 120) \n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(6*17*17, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = x.view(-1, 6*17*17)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 148, 148]              30\n",
      "            Conv2d-2            [-1, 3, 72, 72]              84\n",
      "            Conv2d-3            [-1, 6, 34, 34]             168\n",
      "            Linear-4                    [-1, 2]           3,470\n",
      "================================================================\n",
      "Total params: 3,752\n",
      "Trainable params: 3,752\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net,(1,150,150))\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_epoch = []\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"acc\")\n",
    "\n",
    "y_loss = {}\n",
    "y_loss[\"train\"] = []\n",
    "y_loss[\"test\"] = []\n",
    "y_acc = {}\n",
    "y_acc[\"train\"] = []\n",
    "y_acc[\"test\"] = []\n",
    "#https://sybernix.medium.com/drawing-loss-curves-for-deep-neural-network-training-in-pytorch-ac617b24c388\n",
    "\n",
    "def draw_curve():\n",
    "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n",
    "    ax0.plot(x_epoch, y_loss['test'], 'ro-', label='test')\n",
    "    ax1.plot(x_epoch, y_acc['train'], 'bo-', label='train')\n",
    "    ax1.plot(x_epoch, y_acc['test'], 'ro-', label='test')\n",
    "    file_name = \"train_\" + str(len(x_epoch)) + \".jpg\"\n",
    "    fig.savefig(os.path.join('../lossGraphs_v1', file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 1/20\n",
      "train: Loss: 0.6918 Acc: 0.5541\n",
      "test: Loss: 0.6837 Acc: 0.5974\n",
      "--------------------\n",
      "Epoch 2/20\n",
      "train: Loss: 0.6824 Acc: 0.6190\n",
      "test: Loss: 0.6655 Acc: 0.5584\n",
      "--------------------\n",
      "Epoch 3/20\n",
      "train: Loss: 0.6690 Acc: 0.6349\n",
      "test: Loss: 0.6407 Acc: 0.6623\n",
      "--------------------\n",
      "Epoch 4/20\n",
      "train: Loss: 0.6393 Acc: 0.6667\n",
      "test: Loss: 0.5995 Acc: 0.7532\n",
      "--------------------\n",
      "Epoch 5/20\n",
      "train: Loss: 0.6065 Acc: 0.6797\n",
      "test: Loss: 0.5789 Acc: 0.7662\n",
      "--------------------\n",
      "Epoch 6/20\n",
      "train: Loss: 0.5800 Acc: 0.7085\n",
      "test: Loss: 0.5868 Acc: 0.7013\n",
      "--------------------\n",
      "Epoch 7/20\n",
      "train: Loss: 0.5593 Acc: 0.7215\n",
      "test: Loss: 0.5728 Acc: 0.7532\n",
      "--------------------\n",
      "Epoch 8/20\n",
      "train: Loss: 0.5416 Acc: 0.7302\n",
      "test: Loss: 0.5876 Acc: 0.7143\n",
      "--------------------\n",
      "Epoch 9/20\n",
      "train: Loss: 0.5271 Acc: 0.7374\n",
      "test: Loss: 0.5766 Acc: 0.7143\n",
      "--------------------\n",
      "Epoch 10/20\n",
      "train: Loss: 0.5117 Acc: 0.7590\n",
      "test: Loss: 0.5771 Acc: 0.7273\n",
      "--------------------\n",
      "Epoch 11/20\n",
      "train: Loss: 0.5000 Acc: 0.7662\n",
      "test: Loss: 0.6125 Acc: 0.6494\n",
      "--------------------\n",
      "Epoch 12/20\n",
      "train: Loss: 0.4911 Acc: 0.7677\n",
      "test: Loss: 0.5876 Acc: 0.7273\n",
      "--------------------\n",
      "Epoch 13/20\n",
      "train: Loss: 0.4855 Acc: 0.7633\n",
      "test: Loss: 0.5962 Acc: 0.7143\n",
      "--------------------\n",
      "Epoch 14/20\n",
      "train: Loss: 0.4754 Acc: 0.7778\n",
      "test: Loss: 0.5937 Acc: 0.7273\n",
      "--------------------\n",
      "Epoch 15/20\n",
      "train: Loss: 0.4684 Acc: 0.7763\n",
      "test: Loss: 0.6229 Acc: 0.7143\n",
      "--------------------\n",
      "Epoch 16/20\n",
      "train: Loss: 0.4623 Acc: 0.7792\n",
      "test: Loss: 0.6050 Acc: 0.7273\n",
      "--------------------\n",
      "Epoch 17/20\n",
      "train: Loss: 0.4545 Acc: 0.7807\n",
      "test: Loss: 0.6048 Acc: 0.7403\n",
      "--------------------\n",
      "Epoch 18/20\n",
      "train: Loss: 0.4478 Acc: 0.8052\n",
      "test: Loss: 0.6484 Acc: 0.6623\n",
      "--------------------\n",
      "Epoch 19/20\n",
      "train: Loss: 0.4418 Acc: 0.8023\n",
      "test: Loss: 0.6428 Acc: 0.7403\n",
      "--------------------\n",
      "Epoch 20/20\n",
      "train: Loss: 0.4377 Acc: 0.7980\n",
      "test: Loss: 0.6270 Acc: 0.7403\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print('-' * 20)\n",
    "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    for phase in [\"train\", \"test\"]:     \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0 \n",
    "        if(phase == \"train\"):\n",
    "            net.train(True)\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                images, labels = data\n",
    "                now_batch_size, c, h, w = images.shape\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(images)\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                l = loss(outputs, labels)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += l.item() * now_batch_size\n",
    "                del l\n",
    "                running_corrects += float(torch.sum(predictions == labels.data))\n",
    "        else:\n",
    "            net.train(False)\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                images, labels = data\n",
    "                now_batch_size, c, h, w = images.shape\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(images)\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                l = loss(outputs, labels)\n",
    "\n",
    "                running_loss += l.item() * now_batch_size\n",
    "                del l\n",
    "                running_corrects += float(torch.sum(predictions == labels.data))\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "        print('{}: Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        y_loss[phase].append(epoch_loss)\n",
    "        y_acc[phase].append(epoch_acc)\n",
    "        if(phase == \"test\"):\n",
    "            x_epoch.append(epoch)\n",
    "            draw_curve()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-0.1151, -0.0799,  0.5454],\n",
       "                        [-0.2093, -0.2734,  0.1755],\n",
       "                        [-0.3728,  0.2621, -0.0670]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0938,  0.2398,  0.1491],\n",
       "                        [-0.2567,  0.3145,  0.1904],\n",
       "                        [-0.2756, -0.3029, -0.0900]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3021,  0.5320,  0.2724],\n",
       "                        [-0.1969,  0.5068,  0.5883],\n",
       "                        [-0.1914,  0.5127, -0.0762]]]])),\n",
       "             ('conv1.bias', tensor([-0.1261, -0.1090,  0.0484])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 0.3209,  0.2852,  0.3302],\n",
       "                        [-0.0054,  0.2039,  0.2018],\n",
       "                        [ 0.2615,  0.5314,  0.5129]],\n",
       "              \n",
       "                       [[ 0.1768,  0.1822,  0.0129],\n",
       "                        [-0.1741,  0.1091,  0.0980],\n",
       "                        [-0.0923,  0.1898,  0.0894]],\n",
       "              \n",
       "                       [[-0.3908,  0.0205, -0.1756],\n",
       "                        [ 0.0482,  0.0058,  0.1103],\n",
       "                        [ 0.3685, -0.0058,  0.0279]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1432,  0.1098,  0.1044],\n",
       "                        [ 0.4210,  0.7306,  0.2445],\n",
       "                        [ 0.1438,  0.1653, -0.0595]],\n",
       "              \n",
       "                       [[ 0.1638, -0.0891, -0.1484],\n",
       "                        [ 0.2216,  0.0481,  0.0992],\n",
       "                        [ 0.1461,  0.0435, -0.0064]],\n",
       "              \n",
       "                       [[ 0.1367,  0.3660,  0.0277],\n",
       "                        [ 0.3607,  0.4248,  0.1440],\n",
       "                        [ 0.4222, -0.0294, -0.1294]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0231,  0.0938,  0.0997],\n",
       "                        [ 0.0166,  0.4511,  0.3443],\n",
       "                        [ 0.2607,  0.6336,  0.3592]],\n",
       "              \n",
       "                       [[ 0.1698, -0.1512,  0.1338],\n",
       "                        [ 0.0450,  0.2543,  0.0424],\n",
       "                        [-0.1316,  0.3123,  0.1574]],\n",
       "              \n",
       "                       [[-0.1826, -0.0139,  0.2576],\n",
       "                        [-0.1250,  0.4284,  0.1540],\n",
       "                        [ 0.1434,  0.4922,  0.4119]]]])),\n",
       "             ('conv2.bias', tensor([ 0.0820,  0.0655, -0.0227])),\n",
       "             ('conv3.weight',\n",
       "              tensor([[[[ 0.1351,  0.3289, -0.0297],\n",
       "                        [-0.0397,  0.2544, -0.0184],\n",
       "                        [-0.0224,  0.1890,  0.2148]],\n",
       "              \n",
       "                       [[-0.0403,  0.1734,  0.0505],\n",
       "                        [ 0.0373,  0.0040, -0.1475],\n",
       "                        [-0.1518,  0.0173,  0.0665]],\n",
       "              \n",
       "                       [[ 0.1856,  0.3304,  0.0221],\n",
       "                        [ 0.0370,  0.0207,  0.0622],\n",
       "                        [-0.1131,  0.1606, -0.1582]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0542, -0.1051, -0.0386],\n",
       "                        [ 0.1268,  0.0837,  0.3915],\n",
       "                        [ 0.1023,  0.0157,  0.0579]],\n",
       "              \n",
       "                       [[-0.1621, -0.2969, -0.1387],\n",
       "                        [-0.2400, -0.0740,  0.2050],\n",
       "                        [-0.1125,  0.2950,  0.1987]],\n",
       "              \n",
       "                       [[-0.3698, -0.1349, -0.2743],\n",
       "                        [-0.1837,  0.1875,  0.1358],\n",
       "                        [-0.1338,  0.1666, -0.0561]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1505,  0.1889,  0.0238],\n",
       "                        [-0.1192,  0.0393, -0.2060],\n",
       "                        [ 0.1308,  0.2279,  0.0836]],\n",
       "              \n",
       "                       [[ 0.1774, -0.0758,  0.1923],\n",
       "                        [ 0.2619,  0.0106, -0.1441],\n",
       "                        [ 0.0703, -0.0811, -0.0114]],\n",
       "              \n",
       "                       [[-0.0360,  0.0500,  0.1125],\n",
       "                        [-0.0918,  0.2390, -0.1140],\n",
       "                        [ 0.3062,  0.1567,  0.1010]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0935,  0.1155, -0.1370],\n",
       "                        [ 0.0030, -0.0009,  0.1686],\n",
       "                        [ 0.0152,  0.1346, -0.0932]],\n",
       "              \n",
       "                       [[-0.2564, -0.1400, -0.0005],\n",
       "                        [ 0.0442,  0.2807, -0.0762],\n",
       "                        [ 0.0115,  0.1750,  0.0431]],\n",
       "              \n",
       "                       [[-0.1267,  0.1268,  0.2731],\n",
       "                        [-0.0764,  0.1637, -0.0032],\n",
       "                        [ 0.2549,  0.3704,  0.0600]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1042,  0.0999, -0.0972],\n",
       "                        [-0.1998,  0.1056,  0.1581],\n",
       "                        [ 0.0173, -0.1538,  0.0463]],\n",
       "              \n",
       "                       [[ 0.1819, -0.0288, -0.1735],\n",
       "                        [-0.1273, -0.1661,  0.2080],\n",
       "                        [-0.2121,  0.1174, -0.1902]],\n",
       "              \n",
       "                       [[-0.0009,  0.0027,  0.3558],\n",
       "                        [-0.0160, -0.0469,  0.3731],\n",
       "                        [-0.0119, -0.0307, -0.0230]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2259,  0.1293,  0.1631],\n",
       "                        [-0.0578,  0.0618, -0.2161],\n",
       "                        [-0.0742,  0.0746, -0.0086]],\n",
       "              \n",
       "                       [[ 0.0375, -0.1200,  0.1621],\n",
       "                        [-0.0932,  0.1530,  0.0609],\n",
       "                        [-0.0293,  0.2475, -0.2049]],\n",
       "              \n",
       "                       [[ 0.2470, -0.1351,  0.3658],\n",
       "                        [-0.1866, -0.0854,  0.1769],\n",
       "                        [ 0.3299, -0.0328,  0.0091]]]])),\n",
       "             ('conv3.bias',\n",
       "              tensor([-0.0893, -0.0467, -0.1138, -0.1354,  0.1581, -0.0396])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.0242, -0.0841, -0.0194,  ..., -0.0352,  0.0235,  0.0353],\n",
       "                      [-0.0286,  0.1060,  0.0110,  ...,  0.0360,  0.0090, -0.0390]])),\n",
       "             ('fc3.bias', tensor([-0.0619,  0.1048]))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model.pth\")\n",
    "\n",
    "# newmodel = Multiclass()\n",
    "# with open(\"model.pickle\", \"rb\") as fp:\n",
    "#     newmodel.load_state_dict(pickle.load(fp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
